# Collaborative Evaluation Template

ABOUTME: Template for multi-agent collaborative evaluation experiments
ABOUTME: Provides structure for individual analysis, collaborative synthesis, and attribution tracking

## Evaluation Task
**Task Description**: [Brief description of what's being evaluated]
**Evaluation Criteria**: [Key criteria for assessment]
**Participants**: [List of agent types and their domain expertise]

---

## Individual Agent Analysis

### [agent-name-1] Scratchpad
**Domain**: [e.g., UX Design & Accessibility]
**Independent Analysis**:
[Private thinking space - agent's individual assessment without influence from others]

**Domain-Specific Ranking** (if applicable):
1. [Item 1] - [Brief reasoning]
2. [Item 2] - [Brief reasoning]
...

**Key Insights**:
- [Unique perspective from this domain]
- [Critical concerns or advantages identified]

---

### [agent-name-2] Scratchpad
**Domain**: [e.g., Strategic Game Design]
**Independent Analysis**:
[Private thinking space - agent's individual assessment without influence from others]

**Domain-Specific Ranking** (if applicable):
1. [Item 1] - [Brief reasoning]
2. [Item 2] - [Brief reasoning]
...

**Key Insights**:
- [Unique perspective from this domain]
- [Critical concerns or advantages identified]

---

[Repeat for each agent...]

---

## Collaborative Synthesis

### Cross-Domain Discussion
**Instructions**: Each agent should tag their contributions with [agent-name] for attribution

#### Item Analysis
**[Item Name]**:
- [agent-name]: [Perspective/insight from their domain]
- [agent-name]: [Building on previous point or offering different perspective]
- [agent-name]: [Additional considerations from their domain]

**[Next Item Name]**:
- [agent-name]: [Analysis]
...

### Consensus Building

#### Areas of Agreement
- [What multiple agents agree on across domains]
- [Shared insights that emerged from discussion]

#### Areas of Disagreement  
- [Where domain perspectives conflict]
- [Trade-offs identified between different priorities]

#### Emergent Insights
- [New understanding that emerged from cross-domain synthesis]
- [Patterns or principles discovered through collaboration]

### Final Collaborative Assessment

#### Tier 1: [Category Name]
1. **[Item]** - [Consensus reasoning across domains]
   - Strengths: [Multi-domain validation]
   - Considerations: [Acknowledged limitations or requirements]

#### Tier 2: [Category Name]
[Continue pattern...]

#### Tier 3: [Category Name]
[Continue pattern...]

---

## Methodology Notes

### Collaboration Process
- [How the discussion evolved]
- [Which perspectives were most influential]
- [Any workflow or process observations]

### Attribution Summary
- [Summary of each agent's key contributions]
- [How different domain expertise enhanced the evaluation]

### Experimental Observations
- [Notes about the collaborative process itself]
- [Insights about multi-agent evaluation methodology]

---

**Template Version**: 1.0
**Created**: [Date]
**For Use In**: Multi-agent collaborative evaluation experiments